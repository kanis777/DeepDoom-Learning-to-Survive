# ğŸ§ ğŸ’¥ DeepDoom: Reinforcement Learning with ViZDoom

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.12.4-blue?logo=python">
  <img src="https://img.shields.io/badge/PyTorch-2.2.2-red?logo=pytorch">
  <img src="https://img.shields.io/badge/Gym-0.26.2-brightgreen?logo=openai">
  <img src="https://img.shields.io/badge/Stable--Baselines3-2.2.1-blueviolet?logo=github">
  <img src="https://img.shields.io/badge/ViZDoom-1.2.0-lightgrey?logo=doom">
  <img src="https://img.shields.io/badge/WandB-Logging-yellow?logo=weightsandbiases">
</p>

## ğŸ¯ Project Overview

**DeepDoom** is a deep reinforcement learning project built on top of [ViZDoom](https://github.com/mwydmuch/ViZDoom), a Doom-based RL platform. This project explores the training and evaluation of AI agents using **PPO**, **A2C**, and **DQN** algorithms in complex FPS game scenarios.

The goal is to teach agents how to navigate, aim, survive, and eliminate enemies across different challenge levels in Doom.

> ğŸ”§ Built with custom Gym-compatible environments, visualization, model evaluation, curriculum learning, and a Flask web UI for real-time interaction with trained agents.

---

## ğŸ§  RL Algorithms Implemented

| Algorithm | Type           | Policy Type | Notes |
|----------|----------------|-------------|-------|
| **PPO**  | On-policy      | Stochastic  | Proximal Policy Optimization |
| **A2C**  | On-policy      | Stochastic  | Advantage Actor-Critic |
| **DQN**  | Off-policy     | Deterministic | Deep Q-Network |

---

## ğŸ—ºï¸ Scenarios Implemented

## ğŸ§  Problem Statements / Scenarios

### ğŸŸ© `basic.cfg`
> The purpose of the scenario is just to check if using this framework to train some AI in a 3D environment is feasible.  
> Map is a rectangle with gray walls, ceiling and floor. The player is spawned along the longer wall, in the center. A red, circular monster is spawned randomly somewhere along the opposite wall.  
> Player can only move left, move right, or shoot. One hit is enough to kill the monster. The episode finishes when the monster is killed or times out.

- **Rewards:**
  - ğŸŸ¥ +101 for killing the monster  
  - âŒ -5 for missing a shot  
  - â³ Living reward = -1 per step  
- **Buttons:** Move Left, Move Right, Shoot (3 actions)  
- **Timeout:** 300 steps

---

### ğŸŸ¨ `defend_the_center.cfg`
> The purpose of this scenario is to teach the agent that killing monsters is GOOD, and getting killed by monsters is BAD.  
> Wasting ammunition is also discouraged.  
> The agent is only rewarded for killing monsters â€” it must learn to survive and manage its ammo.  
> The player is spawned at the center of a circular map, surrounded by melee-only monsters that respawn after death.  
> The episode ends when the player dies (inevitable due to limited ammo).

- **Rewards:**
  - ğŸ”« +1 for killing a monster  
  - ğŸ’€ Death penalty = -1  
- **Buttons:** Turn Left, Turn Right, Shoot (3 actions)

---

### ğŸŸ¥ `deadly_corridor.cfg` (âš ï¸ Curriculum Learning)
> The purpose of this scenario is to teach the agent to **navigate toward its goal (a green vest)** while surviving.  
> The map is a corridor filled with **6 shooting monsters**.  
> The player is rewarded for getting closer to the vest and penalized for going backward.  
> If the player ignores monsters and charges ahead, it will likely be killed before reaching the goal.  
> To make survival necessary, the difficulty level is set using `doom_skill = 5`.

- **Rewards:**
  - â• +dX for getting closer to the vest  
  - â– -dX for moving away from the vest  
  - ğŸ’€ Death penalty = -100  
- **Buttons:** Turn Left, Turn Right, Move Left, Move Right, Shoot (5 actions)  
- **Timeout:** 4200 steps  
- **Difficulty:** `doom_skill = 5`

#### ğŸ“ˆ Curriculum Learning Strategy:
Trained in **5 stages** using:
- `deadly_corridor_s1.cfg`
- `deadly_corridor_s2.cfg`
- `deadly_corridor_s3.cfg`
- `deadly_corridor_s4.cfg`
- `deadly_corridor_s5.cfg`  
Each level increases difficulty. Training continues from one stageâ€™s model to the next with total **200,000 steps**.

---

## ğŸ§ª Training Overview

| Scenario           | Algorithms Used         | Training Steps |
|--------------------|-------------------------|----------------|
| basic.cfg          | PPO, A2C, DQN, Hardcoded PPO | 60,000         |
| defend_the_center  | PPO                     | 100,000        |
| deadly_corridor    | PPO (Curriculum Learning)| 200,000 total  |

**Training was visualized with WandB and evaluated using SB3â€™s `evaluate_policy()` utility.**

---

## ğŸŒ Flask Web UI

A simple Flask web interface allows you to:
- Visualize how each model works after the training 
- Interact with agents in real time

### ğŸŒ² Folder Structure

DeepDoom/
â”œâ”€â”€ main/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ basic_env.py
â”‚   â”œâ”€â”€ basic_hardcode.py
â”‚   â”œâ”€â”€ defend_env.py
â”‚   â”œâ”€â”€ deadly_env.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ basic_ppo_1.zip
â”‚   â”‚   â”œâ”€â”€ deadly_ppo_1.zip
â”‚   â”‚   â”œâ”€â”€ defend_ppo_1.zip
â”‚   â”‚   â”œâ”€â”€ basic_dqn.zip
â”‚   â”‚   â”œâ”€â”€ basic_a2c.zip
â”‚   â”‚   â””â”€â”€ ppo_vizdoom_best.ppt
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ style.css
â”œâ”€â”€ deep_doom.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ› ï¸ How to Run

1. **Clone the repo**  
   ```bash
   git clone https://github.com/your-username/DeepDoom.git
   cd DeepDoom```
2. **Install dependencies**

```bash
pip install -r requirements.txt```

3. **Launch Flask App**

```bash
cd main
python app.py```

4. **Explore Notebooks**

deep_doom.ipynb contains training logic and visualization.
